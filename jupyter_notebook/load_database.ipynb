{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64e6b84d",
   "metadata": {},
   "source": [
    "Setting path of database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21164aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, ast, numpy as np, pandas as pd, wfdb\n",
    "from scipy.signal import butter, filtfilt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# === SỬA đường dẫn này cho đúng máy của bạn ===\n",
    "PTBXL_ROOT = \"../database/physionet.org/files/ptb-xl/1.0.1\"  # nhớ sửa <user>\n",
    "METADATA_CSV = os.path.join(PTBXL_ROOT, \"ptbxl_database.csv\")  # metadata\n",
    "# 100 Hz dùng cột filename_lr, sẽ join với PTBXL_ROOT nên không cần WFDB_DIR riêng\n",
    "\n",
    "assert os.path.exists(METADATA_CSV), \"Sai đường dẫn METADATA_CSV\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa93403e",
   "metadata": {},
   "source": [
    "PTB-XL load & lọc nhãn (Lead I, chỉ SR vs AFIB, cân bằng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4131b440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tổng mẫu sau lọc label: 19569 — SR: 18055 AFIB: 1514\n",
      "Một record Lead I: (1000,)\n"
     ]
    }
   ],
   "source": [
    "# 1.1 Đọc metadata\n",
    "Y = pd.read_csv(METADATA_CSV)\n",
    "\n",
    "# 1.2 Hàm lấy nhãn SR/AFIB từ cột scp_codes (kiểu chuỗi dict)\n",
    "# PTB-XL dùng mã chẩn đoán: AFIB cho rung nhĩ; SR có thể được gán là 'SR', 'NSR' hoặc 'NORM' tuỳ phiên bản.\n",
    "def label_from_scp_codes(scp_str):\n",
    "    d = ast.literal_eval(scp_str) if isinstance(scp_str, str) else {}\n",
    "    keys = set(d.keys())\n",
    "    if \"AFIB\" in keys:\n",
    "        return \"AFIB\"\n",
    "    # các khả năng khác nhau cho sinus rhythm\n",
    "    for k in (\"SR\", \"NSR\", \"NORM\", \"SINUS\"):\n",
    "        if k in keys:\n",
    "            return \"SR\"\n",
    "    return None\n",
    "\n",
    "Y[\"target\"] = Y[\"scp_codes\"].apply(label_from_scp_codes)\n",
    "df = Y[Y[\"target\"].isin([\"SR\", \"AFIB\"])].copy()\n",
    "\n",
    "print(\"Tổng mẫu sau lọc label:\", len(df), \"— SR:\", (df.target==\"SR\").sum(), \"AFIB:\", (df.target==\"AFIB\").sum())\n",
    "\n",
    "# 1.3 Nạp sóng thô 100 Hz và lấy LEAD I\n",
    "def read_lead_I_row(row):\n",
    "    # filename_lr là đường dẫn tương đối tới bản ghi 100 Hz (không có đuôi .hea/.dat)\n",
    "    rec_rel  = row[\"filename_lr\"]                     # ví dụ: records100/00000/00001_lr\n",
    "    rec_path = os.path.join(PTBXL_ROOT, rec_rel)\n",
    "\n",
    "    # Ưu tiên rdrecord vì có .sig_name\n",
    "    try:\n",
    "        rec = wfdb.rdrecord(rec_path, physical=True)\n",
    "        sig = rec.p_signal.astype(np.float32)         # (T, n_leads)\n",
    "        lead_names = [s.upper() for s in rec.sig_name]\n",
    "    except Exception:\n",
    "        # Fallback: rdsamp trả (signals, fields_dict)\n",
    "        sig, fields = wfdb.rdsamp(rec_path)\n",
    "        sig = sig.astype(np.float32)\n",
    "        lead_names = [s.upper() for s in fields.get(\"sig_name\", [])]\n",
    "\n",
    "    # Tìm chỉ số Lead I (dự phòng vài biến thể tên)\n",
    "    idx = 0\n",
    "    for cand in (\"I\", \"LEAD I\", \"MLI\", \"MLII\"):  # 'I' là chuẩn; các cand khác chỉ để phòng hờ\n",
    "        if cand in lead_names:\n",
    "            idx = lead_names.index(cand)\n",
    "            break\n",
    "\n",
    "    return sig[:, idx]\n",
    "\n",
    "# Ví dụ: đọc thử 1 bản ghi\n",
    "x_demo = read_lead_I_row(df.iloc[0])\n",
    "print(\"Một record Lead I:\", x_demo.shape)\n",
    "\n",
    "# 1.4 (Sau này) khi đã có beats, ta sẽ cân bằng lớp bằng downsampling. Ở bước này mới lọc record theo nhãn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eef07ee",
   "metadata": {},
   "source": [
    "Hàm xử lý tín hiệu & R-peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34493617",
   "metadata": {},
   "outputs": [],
   "source": [
    "FS = 100.0  # 100 Hz\n",
    "\n",
    "def butter_highpass(data, fc=0.5, order=4, fs=FS):\n",
    "    nyq = 0.5 * fs\n",
    "    b, a = butter(order, fc/nyq, btype='high')\n",
    "    return filtfilt(b, a, data)\n",
    "\n",
    "def butter_lowpass(data, fc=41.0, order=3, fs=FS):\n",
    "    nyq = 0.5 * fs\n",
    "    b, a = butter(order, fc/nyq, btype='low')\n",
    "    return filtfilt(b, a, data)\n",
    "\n",
    "def preprocess_record(x):\n",
    "    x = butter_highpass(x, 0.5, 4, FS)\n",
    "    x = butter_lowpass(x, 41.0, 3, FS)\n",
    "    return x\n",
    "\n",
    "def detect_r_peaks(x, fs=FS, thresh_frac=0.5, refractory_ms=200):\n",
    "    x = np.asarray(x)\n",
    "    thr = thresh_frac * np.max(np.abs(x))\n",
    "    refr = int(round(refractory_ms * fs / 1000.0))\n",
    "    peaks, last = [], -refr\n",
    "    for i in range(1, len(x)-1):\n",
    "        if x[i] > thr and x[i] >= x[i-1] and x[i] >= x[i+1]:\n",
    "            if i - last >= refr:\n",
    "                peaks.append(i); last = i\n",
    "    return np.array(peaks, dtype=int)\n",
    "\n",
    "PRE, POST = 40, 60\n",
    "def extract_beats(x, rpos, pre=PRE, post=POST):\n",
    "    beats = []\n",
    "    for r in rpos:\n",
    "        a, b = r-pre, r+post\n",
    "        if a >= 0 and b <= len(x):\n",
    "            beats.append(x[a:b])\n",
    "    return np.asarray(beats, dtype=np.float32)\n",
    "\n",
    "def normalize_beats(X):\n",
    "    Xn = np.empty_like(X)\n",
    "    for i, b in enumerate(X):\n",
    "        m, s = b.mean(), b.std() + 1e-8\n",
    "        Xn[i] = (b - m) / s\n",
    "    return Xn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee3fa60",
   "metadata": {},
   "source": [
    "Biến toàn bộ record → beats + label + (tuỳ chọn) patient_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "082037b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/3.11.9/envs/afib311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 19569/19569 [01:02<00:00, 315.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beats tổng: (217587, 100)  — SR: 198371  AFIB: 19216\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "all_beats, all_labels, all_patients = [], [], []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    try:\n",
    "        x = preprocess_record(read_lead_I_row(row))   # dùng hàm bạn đã có\n",
    "        rpos = detect_r_peaks(x)\n",
    "        beats = extract_beats(x, rpos)\n",
    "        if len(beats) == 0:\n",
    "            continue\n",
    "        beats = normalize_beats(beats)\n",
    "\n",
    "        all_beats.append(beats)\n",
    "        y_val = 1 if row[\"target\"] == \"AFIB\" else 0\n",
    "        all_labels.append(np.full(len(beats), y_val, dtype=np.int64))\n",
    "        pid = row.get(\"patient_id\", row.get(\"ecg_id\", -1))\n",
    "        all_patients.append(np.full(len(beats), pid))\n",
    "    except Exception as e:\n",
    "        # nếu có record lỗi, bỏ qua để pipeline không dừng\n",
    "        # print(\"skip\", row.get(\"ecg_id\", \"?\"), e)\n",
    "        pass\n",
    "\n",
    "X = np.vstack(all_beats)               # (N_beats, 100)\n",
    "y = np.concatenate(all_labels)         # (N_beats,)\n",
    "patients = np.concatenate(all_patients)\n",
    "\n",
    "print(\"Beats tổng:\", X.shape, \" — SR:\", (y==0).sum(), \" AFIB:\", (y==1).sum())\n",
    "# thêm trục kênh cho Conv1D\n",
    "X = X[..., None]                        # -> (N_beats, 100, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f01802",
   "metadata": {},
   "source": [
    "Cân bằng lớp theo beat (downsample lớp nhiều hơn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bedd273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sau cân bằng: (38432, 100, 1) — mỗi lớp: 19216 19216\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(123)\n",
    "idx0 = np.where(y==0)[0]\n",
    "idx1 = np.where(y==1)[0]\n",
    "m = min(len(idx0), len(idx1))\n",
    "idx0 = rng.choice(idx0, size=m, replace=False)\n",
    "idx1 = rng.choice(idx1, size=m, replace=False)\n",
    "sel = np.sort(np.concatenate([idx0, idx1]))\n",
    "\n",
    "Xb, yb, patients_b = X[sel], y[sel], patients[sel]\n",
    "print(\"Sau cân bằng:\", Xb.shape, \"— mỗi lớp:\", (yb==0).sum(), (yb==1).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ed4cea",
   "metadata": {},
   "source": [
    "Chia tập"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a449edbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30746, 100, 1), (7686, 100, 1))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_by_beats(X, y, val_ratio=0.2, seed=123):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    N = len(X); perm = rng.permutation(N)\n",
    "    nval = int(round(val_ratio*N))\n",
    "    val_idx, tr_idx = perm[:nval], perm[nval:]\n",
    "    return (X[tr_idx], y[tr_idx]), (X[val_idx], y[val_idx])\n",
    "\n",
    "# (hoặc) theo patient:\n",
    "def split_by_patient(X, y, patients, val_ratio=0.2, seed=123):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    uniq = np.unique(patients); rng.shuffle(uniq)\n",
    "    nval = int(round(val_ratio*len(uniq)))\n",
    "    val_p = set(uniq[:nval]); tr_p = set(uniq[nval:])\n",
    "    tr_idx = np.where(np.isin(patients, list(tr_p)))[0]\n",
    "    val_idx = np.where(np.isin(patients, list(val_p)))[0]\n",
    "    return (X[tr_idx], y[tr_idx]), (X[val_idx], y[val_idx])\n",
    "\n",
    "# chọn 1 trong 2:\n",
    "(Xtr, ytr), (Xva, yva) = split_by_beats(Xb, yb, 0.2, 123)\n",
    "# (Xtr, ytr), (Xva, yva) = split_by_patient(Xb, yb, patients_b, 0.2, 123)\n",
    "\n",
    "Xtr.shape, Xva.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693d5cb4",
   "metadata": {},
   "source": [
    "Train 1D-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f79b17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 04:15:29.202786: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-08-23 04:15:29.593976: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-08-23 04:15:29.594121: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-08-23 04:15:29.671443: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-08-23 04:15:29.834609: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-08-23 04:15:29.837922: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-23 04:15:31.515008: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 100, 1)]          0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 100, 16)           96        \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, 50, 16)            0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 50, 32)            2592      \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPoolin  (None, 25, 32)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 25, 32)            0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 800)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                25632     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28386 (110.88 KB)\n",
      "Trainable params: 28386 (110.88 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 04:15:32.892866: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-08-23 04:15:32.894679: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241/241 - 2s - loss: 0.3311 - accuracy: 0.8585 - val_loss: 0.2615 - val_accuracy: 0.8968 - 2s/epoch - 8ms/step\n",
      "Epoch 2/25\n",
      "241/241 - 1s - loss: 0.2457 - accuracy: 0.9072 - val_loss: 0.2319 - val_accuracy: 0.9128 - 1s/epoch - 5ms/step\n",
      "Epoch 3/25\n",
      "241/241 - 1s - loss: 0.2295 - accuracy: 0.9130 - val_loss: 0.2218 - val_accuracy: 0.9183 - 1s/epoch - 5ms/step\n",
      "Epoch 4/25\n",
      "241/241 - 1s - loss: 0.2193 - accuracy: 0.9173 - val_loss: 0.2177 - val_accuracy: 0.9182 - 1s/epoch - 5ms/step\n",
      "Epoch 5/25\n",
      "241/241 - 1s - loss: 0.2117 - accuracy: 0.9215 - val_loss: 0.2140 - val_accuracy: 0.9188 - 1s/epoch - 5ms/step\n",
      "Epoch 6/25\n",
      "241/241 - 1s - loss: 0.2061 - accuracy: 0.9227 - val_loss: 0.2114 - val_accuracy: 0.9215 - 1s/epoch - 5ms/step\n",
      "Epoch 7/25\n",
      "241/241 - 1s - loss: 0.1997 - accuracy: 0.9265 - val_loss: 0.2068 - val_accuracy: 0.9228 - 1s/epoch - 5ms/step\n",
      "Epoch 8/25\n",
      "241/241 - 1s - loss: 0.1944 - accuracy: 0.9271 - val_loss: 0.2045 - val_accuracy: 0.9238 - 1s/epoch - 6ms/step\n",
      "Epoch 9/25\n",
      "241/241 - 1s - loss: 0.1900 - accuracy: 0.9294 - val_loss: 0.2031 - val_accuracy: 0.9261 - 1s/epoch - 5ms/step\n",
      "Epoch 10/25\n",
      "241/241 - 1s - loss: 0.1872 - accuracy: 0.9297 - val_loss: 0.1988 - val_accuracy: 0.9274 - 1s/epoch - 6ms/step\n",
      "Epoch 11/25\n",
      "241/241 - 1s - loss: 0.1838 - accuracy: 0.9317 - val_loss: 0.2012 - val_accuracy: 0.9275 - 1s/epoch - 6ms/step\n",
      "Epoch 12/25\n",
      "241/241 - 2s - loss: 0.1798 - accuracy: 0.9323 - val_loss: 0.1989 - val_accuracy: 0.9275 - 2s/epoch - 7ms/step\n",
      "Epoch 13/25\n",
      "241/241 - 2s - loss: 0.1766 - accuracy: 0.9332 - val_loss: 0.1941 - val_accuracy: 0.9297 - 2s/epoch - 7ms/step\n",
      "Epoch 14/25\n",
      "241/241 - 2s - loss: 0.1719 - accuracy: 0.9357 - val_loss: 0.1906 - val_accuracy: 0.9305 - 2s/epoch - 7ms/step\n",
      "Epoch 15/25\n",
      "241/241 - 1s - loss: 0.1706 - accuracy: 0.9352 - val_loss: 0.1937 - val_accuracy: 0.9299 - 1s/epoch - 6ms/step\n",
      "Epoch 16/25\n",
      "241/241 - 1s - loss: 0.1651 - accuracy: 0.9382 - val_loss: 0.1911 - val_accuracy: 0.9297 - 1s/epoch - 6ms/step\n",
      "Epoch 17/25\n",
      "241/241 - 1s - loss: 0.1629 - accuracy: 0.9387 - val_loss: 0.1936 - val_accuracy: 0.9304 - 1s/epoch - 5ms/step\n",
      "Epoch 18/25\n",
      "241/241 - 1s - loss: 0.1606 - accuracy: 0.9402 - val_loss: 0.1891 - val_accuracy: 0.9344 - 1s/epoch - 5ms/step\n",
      "Epoch 19/25\n",
      "241/241 - 1s - loss: 0.1569 - accuracy: 0.9406 - val_loss: 0.1920 - val_accuracy: 0.9301 - 1s/epoch - 5ms/step\n",
      "Epoch 20/25\n",
      "241/241 - 1s - loss: 0.1566 - accuracy: 0.9422 - val_loss: 0.1906 - val_accuracy: 0.9297 - 1s/epoch - 5ms/step\n",
      "Epoch 21/25\n",
      "241/241 - 1s - loss: 0.1533 - accuracy: 0.9434 - val_loss: 0.1967 - val_accuracy: 0.9310 - 1s/epoch - 5ms/step\n",
      "Epoch 22/25\n",
      "241/241 - 1s - loss: 0.1511 - accuracy: 0.9436 - val_loss: 0.2067 - val_accuracy: 0.9264 - 1s/epoch - 5ms/step\n",
      "Epoch 23/25\n",
      "241/241 - 1s - loss: 0.1487 - accuracy: 0.9444 - val_loss: 0.1950 - val_accuracy: 0.9283 - 1s/epoch - 6ms/step\n",
      "Epoch 24/25\n",
      "241/241 - 1s - loss: 0.1459 - accuracy: 0.9448 - val_loss: 0.1933 - val_accuracy: 0.9299 - 1s/epoch - 5ms/step\n",
      "Epoch 25/25\n",
      "241/241 - 1s - loss: 0.1429 - accuracy: 0.9460 - val_loss: 0.1906 - val_accuracy: 0.9317 - 1s/epoch - 5ms/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def build_model(input_len=100, n_classes=2):\n",
    "    inp = keras.Input(shape=(input_len, 1))\n",
    "    x = layers.Conv1D(16, 5, padding='same', activation='relu')(inp)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Conv1D(32, 5, padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    out = layers.Dense(n_classes, activation='softmax')(x)\n",
    "    model = keras.Model(inp, out)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(1e-3),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()\n",
    "\n",
    "hist = model.fit(Xtr, ytr, epochs=25, batch_size=128,\n",
    "                 validation_data=(Xva, yva), verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd84b84",
   "metadata": {},
   "source": [
    "Đánh giá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8be6fca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9316939890710383\n",
      "F1: 0.933552714846222\n",
      "[[3473  302]\n",
      " [ 223 3688]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          SR       0.94      0.92      0.93      3775\n",
      "        AFIB       0.92      0.94      0.93      3911\n",
      "\n",
      "    accuracy                           0.93      7686\n",
      "   macro avg       0.93      0.93      0.93      7686\n",
      "weighted avg       0.93      0.93      0.93      7686\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "yp = model.predict(Xva, verbose=0).argmax(axis=1)\n",
    "print(\"Accuracy:\", accuracy_score(yva, yp))\n",
    "print(\"F1:\", f1_score(yva, yp))\n",
    "print(confusion_matrix(yva, yp))\n",
    "print(classification_report(yva, yp, target_names=[\"SR\",\"AFIB\"]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "afib311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
